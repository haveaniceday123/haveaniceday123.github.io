I"Áq<p>æ˜¾å¡ä¸Šçš„è§„çº¦æ“ä½œæ˜¯ä¸€ä¸ªç»å…¸ä¼˜åŒ–æ¡ˆä¾‹ã€‚åœ¨ç½‘ä¸Šèƒ½æ‰¾åˆ°çš„å¤§éƒ¨åˆ†å®ç°ä¸­ï¼Œæ€§èƒ½æ¯”è¾ƒä¼˜ç§€çš„æ˜¯ä½¿ç”¨ Shared Memory å¹¶è¿›è¡Œè®¿å­˜ä¼˜åŒ–çš„æ ‘å½¢è§„çº¦ã€‚</p>

<p>è¿‘æœŸæ­£å¥½åœ¨åšè¿™æ–¹é¢çš„ä¸€äº›ä¼˜åŒ–ï¼ŒåŒæ—¶äº†è§£åˆ°ä» CUDA 9.0 å¼€å§‹ï¼ŒCUDA å¼•å…¥äº†æ›´åŠ çµæ´»çš„ Warp æ“ä½œåŸè¯­ï¼Œè¿™ä¸€æ–¹é¢ä½¿å¾— CUDA ç¼–ç¨‹æ›´åŠ ç®€å•ï¼Œä¸€æ–¹é¢ä¹Ÿä½¿å¾—ä¸€äº›åŸæœ‰çš„åŠŸèƒ½å‘ç”Ÿäº†ä¸€äº›æ”¹å˜ã€‚æœ¬æ–‡é‡ç‚¹å¯¹ Warp å’Œ Shared Memory ä¸¤ç§æ–¹æ³•å®ç°çš„å¹¶è¡Œè§„çº¦æ“ä½œè¿›è¡Œæ€§èƒ½å¯¹æ¯”ã€‚</p>

<h2 id="å®éªŒç¯å¢ƒ">å®éªŒç¯å¢ƒ</h2>

<p>ä½¿ç”¨ v100 é›†ç¾¤ä¸Šä¸€ä¸ªç»“ç‚¹çš„å•å¼  v100 è¿è¡Œã€‚</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>nvdia-smi
Mon Dec  2 08:38:49 2019
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.48                 Driver Version: 410.48                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|<span class="o">===============================</span>+<span class="o">======================</span>+<span class="o">======================</span>|
|   0  Tesla V100-PCIE...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0    24W / 250W |      0MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|<span class="o">=============================================================================</span>|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre></div></div>

<h2 id="å®éªŒè¿‡ç¨‹ä¸åˆ†æ">å®éªŒè¿‡ç¨‹ä¸åˆ†æ</h2>

<h3 id="å‰ææ¡ä»¶">å‰ææ¡ä»¶</h3>

<p>ä¸ºäº†ç®€åŒ–ä»£ç ï¼Œè¿™é‡Œæˆ‘ä½¿ç”¨ <code class="language-plaintext highlighter-rouge">&lt;thrust/device_vector.h&gt;</code> åº“è¿›è¡Œå†…å­˜ç®¡ç†ã€‚è™½ç„¶ <code class="language-plaintext highlighter-rouge">&lt;thrust&gt;</code> åº“å¯¹åº•å±‚ä»£ç åšäº†æ›´é«˜çº§çš„æŠ½è±¡ï¼Œä½†åœ¨æœ¬ä¾‹ä¸­çš„å¼€é”€å´å‡ ä¹å¯ä»¥å¿½ç•¥ï¼ˆå·²ç»å’Œæ‰‹åŠ¨ <code class="language-plaintext highlighter-rouge">cudaMalloc</code> çš„ä»£ç è¿›è¡Œå¯¹æ¯”éªŒè¯ï¼‰ã€‚</p>

<p>æ­¤å¤–ï¼Œåœ¨æ˜¾å¡ä¸Šè¿›è¡ŒåŒºé—´è§„çº¦æ—¶ï¼Œä¸ºäº†é¿å…åŠ è¯»å†™é”æˆ–è€…åŸå­æ“ä½œï¼Œé€šå¸¸ä¸ç›´æ¥æŠŠæ‰€æœ‰çš„ç»“æœå½’çº¦åˆ°åŒä¸€ä¸ªå†…å­˜åœ°å€ã€‚å–è€Œä»£ä¹‹çš„ï¼Œæ˜¯ä½¿ç”¨å¤šçº§è§„çº¦ï¼Œé€šè¿‡å¤šæ¬¡å¯åŠ¨æ ¸å‡½æ•°ï¼Œåœ¨å‡ ä¹æ²¡æœ‰å¢åŠ å¤šå°‘è®¿å­˜çš„å‰æä¸‹å¤§å¤§å¢åŠ äº†çº¿ç¨‹çš„æ•ˆç‡ã€‚ç„¶è€Œï¼Œåœ¨è¿™é‡Œæˆ‘åªæ˜¯æƒ³å¯¹ Shared Memory å’Œ Warp Shuffle è®¿å­˜çš„æ€§èƒ½è¿›è¡Œå¯¹æ¯”ï¼Œè€Œæ—¶é—´å¤§å¤´å…¶å®éƒ½åœ¨ç¬¬ä¸€å±‚è§„çº¦ä¸Šã€‚äºŒçº§ä¹‹åçš„è§„çº¦å·ä¸ªæ‡’ç”¨<code class="language-plaintext highlighter-rouge">thrust::reduce</code>ä»£æ›¿ä¹‹ï¼ˆå…¶å®åº”è¯¥å¤šæ¬¡å¯åŠ¨è‡ªå·±çš„æ ¸å‡½æ•°ï¼Œä½†æ˜¯æˆ‘å¤ªæ‡’äº†ï¼‰ã€‚ç”±äºäºŒçº§åŠä¹‹åçš„è§„çº¦æ—¶é—´å…¶å®æ˜¯å¯ä»¥å¿½ç•¥ä¸è®¡çš„ï¼Œå› æ­¤åœ¨æœ¬ä¾‹ä¸­æ˜¯å®Œå…¨å¯è¡Œçš„ã€‚</p>

<h3 id="ä½¿ç”¨-shared-memory">ä½¿ç”¨ Shared Memory</h3>

<p>è™½ç„¶ä¸æ˜¯æ–‡ç« é‡ç‚¹ï¼Œä½†æˆ‘è¿˜æ˜¯è§‰å¾—æœ‰å¿…è¦å¤ä¹ ä¸€ä¸‹ Shared Memory ä¸Šè¿›è¡Œ Reduce çš„ä¸€äº› Trick æ“ä½œã€‚</p>

<ul>
  <li>ä½¿ç”¨<code class="language-plaintext highlighter-rouge">template</code>è¿™æ˜¯ä¸ºäº†è®©æ¥ä¸‹æ¥çš„å¾ªç¯<code class="language-plaintext highlighter-rouge">for (size_t offset = BLOCK_SIZE &gt;&gt; 1; offset &gt; 0; offset &gt;&gt;= 1)</code>å¯ä»¥è¢«ç¼–è¯‘å™¨è‡ªåŠ¨å±•å¼€ä¼˜åŒ–ã€‚å¦‚æœç›´æ¥ä½¿ç”¨<code class="language-plaintext highlighter-rouge">blockDim.x</code>çš„è¯ï¼Œæ—¢ä¸èƒ½è®©ç¼–è¯‘å™¨å±•å¼€å¾ªç¯ï¼Œä¹Ÿä¸èƒ½ç”¨ä½œå£°æ˜ <code class="language-plaintext highlighter-rouge">shared[]</code> å¤§å°ã€‚</li>
  <li>å¯¹ reduce è¿‡ç¨‹ä¸­çš„è®¿å­˜è¿›è¡Œä¼˜åŒ–ï¼š<code class="language-plaintext highlighter-rouge">if (reduce_id &lt; offset) shared[threadIdx.x] += shared[threadIdx.x ^ offset];</code>ï¼Œè¿™é‡Œè®¿å­˜çš„æ—¶å€™ç›¸é‚»çº¿ç¨‹è®¿é—®ç›¸é‚»çš„åœ°å€ï¼Œä¹Ÿæ²¡æœ‰ conflictã€‚ï¼ˆè®¿å­˜çš„åŸç†å›¾å’Œä¸‹é¢ Warp çš„é‚£å¼ å¾ˆç±»ä¼¼ï¼Œè¿™é‡Œå°±ä¸æ”¾å‡ºäº†ï¼‰</li>
  <li>å®˜æ–¹çš„ä»£ç è¿˜æœ‰ä¸€äº›æ¯”è¾ƒç»™åŠ²çš„ä¼˜åŒ–ç­–ç•¥ï¼ˆè§æ–‡æœ«çš„å‚è€ƒï¼‰ï¼Œå¦‚ Completely Unrolledã€Multiple Addsã€‚ä½†æ˜¯è¿™äº›ç­–ç•¥éƒ½æ¯”è¾ƒæš´åŠ›ï¼Œä¸”ä¸æ˜¯æœ¬æ–‡é‡ç‚¹ï¼Œä¸æ–¹ä¾¿å’Œ Warp è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘è¿™é‡Œå°±æ²¡æœ‰åšäº†ã€‚</li>
</ul>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="kt">size_t</span> <span class="n">BLOCK_SIZE</span><span class="p">&gt;</span>
<span class="kt">void</span> <span class="n">__global__</span> <span class="nf">shared_asum_kernel</span><span class="p">(</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">unsigned</span> <span class="o">*</span><span class="n">src_d</span><span class="p">,</span>
	<span class="kt">unsigned</span> <span class="o">*</span><span class="n">tmp_d</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">size_t</span>
		<span class="n">global_id</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="n">__shared__</span> <span class="n">shared</span><span class="p">[</span><span class="n">BLOCK_SIZE</span><span class="p">];</span>
	<span class="n">shared</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">src_d</span><span class="p">[</span><span class="n">global_id</span><span class="p">];</span>
	<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">BLOCK_SIZE</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="n">__syncthreads</span><span class="p">();</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">offset</span><span class="p">)</span>
			<span class="n">shared</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+=</span> <span class="n">shared</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">^</span> <span class="n">offset</span><span class="p">];</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
		<span class="n">tmp_d</span><span class="p">[</span><span class="n">global_id</span> <span class="o">/</span> <span class="n">BLOCK_SIZE</span><span class="p">]</span> <span class="o">=</span> <span class="n">shared</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
<span class="p">}</span>
</code></pre></div></div>

<p>è¿è¡Œæ—¶é—´ä¸º<code class="language-plaintext highlighter-rouge">15.728032ms</code>ã€‚</p>

<h3 id="ä½¿ç”¨-warp">ä½¿ç”¨ Warp</h3>

<p>Warp çº§åˆ«çš„æ“ä½œåŸè¯­ï¼ˆWarp-level Primitivesï¼‰é€šè¿‡ shuffle æŒ‡ä»¤ï¼Œå…è®¸ thread ç›´æ¥è¯»å…¶ä»– thread çš„å¯„å­˜å™¨å€¼ï¼Œåªè¦ä¸¤ä¸ª thread åœ¨åŒä¸€ä¸ª warp ä¸­ï¼Œè¿™ç§æ¯”é€šè¿‡ shared Memory è¿›è¡Œ thread é—´çš„é€šè®¯æ•ˆæœæ›´å¥½ï¼Œlatency æ›´ä½ï¼ŒåŒæ—¶ä¹Ÿä¸æ¶ˆè€—é¢å¤–çš„å†…å­˜èµ„æºæ¥æ‰§è¡Œæ•°æ®äº¤æ¢ã€‚å¯ä»¥çœ‹åˆ°ï¼Œå’Œä½¿ç”¨ Shared Memory çš„ä»£ç é•¿å¾—éå¸¸ç›¸ä¼¼ï¼Œåªæ˜¯<code class="language-plaintext highlighter-rouge">BLOCK_SIZE</code>æ¢æˆäº†<code class="language-plaintext highlighter-rouge">WARP_SIZE</code>ï¼Œ<code class="language-plaintext highlighter-rouge">threadIdx.x</code>æ¢æˆäº†<code class="language-plaintext highlighter-rouge">lane_id</code>ã€‚</p>

<p><img src="https://devblogs.nvidia.com/wp-content/uploads/2018/01/reduce_shfl_down.png" alt="Part of a warp-level parallel reduction using shfl_down_sync()." /></p>

<p>æ­¤å¤–ï¼Œæˆ‘ç”¨<code class="language-plaintext highlighter-rouge">__shfl_xor_sync</code>è€Œä¸æ˜¯<code class="language-plaintext highlighter-rouge">__shfl_down_sync</code>ï¼Œè¿™æ ·å®ç°çš„ä¸ä»…ä»…æ˜¯æ ‘å½¢è§„çº¦ï¼Œè¿˜æ˜¯ä¸€ä¸ªè¶å½¢è§„çº¦ï¼å¹¶ä¸”é€šä¿¡æ­¥æ•°å¹¶æ²¡æœ‰å¢åŠ ~</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="kt">size_t</span> <span class="n">WARP_SIZE</span><span class="p">&gt;</span>
<span class="kt">void</span> <span class="n">__global__</span> <span class="nf">warp_asum_kernel</span><span class="p">(</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">unsigned</span> <span class="o">*</span><span class="n">src_d</span><span class="p">,</span>
	<span class="kt">unsigned</span> <span class="o">*</span><span class="n">tmp_d</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">size_t</span>
		<span class="n">global_id</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">,</span>
		<span class="n">lane_id</span> <span class="o">=</span> <span class="n">global_id</span> <span class="o">%</span> <span class="n">WARP_SIZE</span><span class="p">;</span>
	<span class="kt">unsigned</span>
		<span class="n">val</span> <span class="o">=</span> <span class="n">global_id</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">?</span> <span class="n">src_d</span><span class="p">[</span><span class="n">global_id</span><span class="p">]</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">WARP_SIZE</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">)</span>
		<span class="n">val</span> <span class="o">+=</span> <span class="n">__shfl_xor_sync</span><span class="p">(</span><span class="mh">0xffffffff</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">WARP_SIZE</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">lane_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
		<span class="n">tmp_d</span><span class="p">[</span><span class="n">global_id</span> <span class="o">/</span> <span class="n">WARP_SIZE</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>è¿è¡Œæ—¶é—´è¾¾åˆ°äº†<code class="language-plaintext highlighter-rouge">7.712928ms</code>ï¼Œè½»æ¾æé«˜äº†ä¸€å€å¤šçš„è®¡ç®—æ€§èƒ½ï¼åŒæ—¶ä¹Ÿä¸éš¾å‘ç°ï¼Œä½¿ç”¨ Warp æ“ä½œåŸè¯­çš„ä»£ç æ›´ç®€æ´ï¼ŒåŒæ—¶ä¹Ÿç§»é™¤äº†å¯¹ Shared Memory çš„ä¾èµ–ï¼Œå¯ä»¥è¯´æ˜¯éå¸¸æ£’äº†ï¼</p>

<h2 id="æºä»£ç ä¸è¿è¡Œç»“æœ">æºä»£ç ä¸è¿è¡Œç»“æœ</h2>

<h3 id="asumpbs"><code class="language-plaintext highlighter-rouge">asum.pbs</code></h3>

<p>è°ƒåº¦è„šæœ¬ã€‚</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#PBS -N Dasum</span>
<span class="c">#PBS -l nodes=1:ppn=32:gpus=1</span>
<span class="c">#PBS -j oe</span>
<span class="c">#PBS -q gpu</span>
<span class="nb">source</span> /public/software/profile.d/cuda10.0.sh
<span class="nb">cd</span> <span class="nv">$PBS_O_WORKDIR</span>
nvcc asum.cu <span class="nt">-run</span>
</code></pre></div></div>

<h3 id="asumo18854"><code class="language-plaintext highlighter-rouge">asum.o18854</code></h3>

<p>è¿è¡Œç»“æœã€‚</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1073741824 : 5.524512ms elapsed.
1073741824 : 15.728032ms elapsed.
1073741824 : 7.712928ms elapsed.
</code></pre></div></div>

<p>ç¬¬ä¸€è¡Œæ˜¯ä½¿ç”¨ <code class="language-plaintext highlighter-rouge">thrust::reduce</code> åº“çš„ç»“æœï¼Œä½œä¸ºä¸¤ç§ä¼˜åŒ–æ–¹æ¡ˆçš„æ ‡æ†ã€‚å¯ä»¥çœ‹åˆ°ï¼Œ<code class="language-plaintext highlighter-rouge">thrust</code> åº“èƒ½å¤Ÿå·ç§°ã€ŒCode at the speed of lightã€ï¼Œè¿˜æ˜¯åšäº†å¾ˆå¤šä¼˜åŒ–çš„ã€‚</p>

<h3 id="asumcu"><code class="language-plaintext highlighter-rouge">asum.cu</code></h3>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include &lt;stdio.h&gt;
#include &lt;cuda_runtime.h&gt;
#include &lt;thrust/device_vector.h&gt;
#include &lt;thrust/reduce.h&gt;
</span><span class="k">template</span> <span class="o">&lt;</span><span class="kt">size_t</span> <span class="n">BLOCK_SIZE</span><span class="p">&gt;</span>
<span class="kt">void</span> <span class="n">__global__</span> <span class="nf">shared_asum_kernel</span><span class="p">(</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">unsigned</span> <span class="o">*</span><span class="n">src_d</span><span class="p">,</span>
	<span class="kt">unsigned</span> <span class="o">*</span><span class="n">tmp_d</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">size_t</span>
		<span class="n">global_id</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
	<span class="kt">unsigned</span> <span class="n">__shared__</span> <span class="n">shared</span><span class="p">[</span><span class="n">BLOCK_SIZE</span><span class="p">];</span>
	<span class="n">shared</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">src_d</span><span class="p">[</span><span class="n">global_id</span><span class="p">];</span>
	<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">BLOCK_SIZE</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="n">__syncthreads</span><span class="p">();</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">offset</span><span class="p">)</span>
			<span class="n">shared</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+=</span> <span class="n">shared</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">^</span> <span class="n">offset</span><span class="p">];</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
		<span class="n">tmp_d</span><span class="p">[</span><span class="n">global_id</span> <span class="o">/</span> <span class="n">BLOCK_SIZE</span><span class="p">]</span> <span class="o">=</span> <span class="n">shared</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
<span class="p">}</span>
<span class="k">template</span> <span class="o">&lt;</span><span class="kt">size_t</span> <span class="n">WARP_SIZE</span><span class="p">&gt;</span>
<span class="kt">void</span> <span class="n">__global__</span> <span class="nf">warp_asum_kernel</span><span class="p">(</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">unsigned</span> <span class="o">*</span><span class="n">src_d</span><span class="p">,</span>
	<span class="kt">unsigned</span> <span class="o">*</span><span class="n">tmp_d</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">size_t</span>
		<span class="n">global_id</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">,</span>
		<span class="n">lane_id</span> <span class="o">=</span> <span class="n">global_id</span> <span class="o">%</span> <span class="n">WARP_SIZE</span><span class="p">;</span>
	<span class="kt">unsigned</span>
		<span class="n">val</span> <span class="o">=</span> <span class="n">global_id</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">?</span> <span class="n">src_d</span><span class="p">[</span><span class="n">global_id</span><span class="p">]</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">WARP_SIZE</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">)</span>
		<span class="n">val</span> <span class="o">+=</span> <span class="n">__shfl_xor_sync</span><span class="p">(</span><span class="mh">0xffffffff</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">WARP_SIZE</span><span class="p">);</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">lane_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
		<span class="n">tmp_d</span><span class="p">[</span><span class="n">global_id</span> <span class="o">/</span> <span class="n">WARP_SIZE</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
<span class="p">}</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">size_t</span>
		<span class="n">n</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">30</span><span class="p">,</span>
		<span class="n">BLOCK_SIZE</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">10</span><span class="p">,</span>
		<span class="n">WARP_SIZE</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">5</span><span class="p">,</span>
		<span class="n">REDUCE_SIZE</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="n">WARP_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">WARP_SIZE</span><span class="p">;</span>
	<span class="n">thrust</span><span class="o">::</span><span class="n">device_vector</span><span class="o">&lt;</span><span class="kt">unsigned</span><span class="o">&gt;</span> <span class="n">src</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tmp</span><span class="p">(</span><span class="n">REDUCE_SIZE</span><span class="p">);</span>
	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">op</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">op</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">;</span> <span class="o">++</span><span class="n">op</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="kt">unsigned</span> <span class="n">sum</span><span class="p">;</span>
		<span class="n">cudaEvent_t</span> <span class="n">beg</span><span class="p">,</span> <span class="n">end</span><span class="p">;</span>
		<span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">beg</span><span class="p">);</span>
		<span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">end</span><span class="p">);</span>
		<span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">beg</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">op</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
			<span class="n">sum</span> <span class="o">=</span> <span class="n">thrust</span><span class="o">::</span><span class="n">reduce</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">src</span><span class="p">.</span><span class="n">begin</span><span class="p">()</span> <span class="o">+</span> <span class="n">n</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">op</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
		<span class="p">{</span>
			<span class="n">shared_asum_kernel</span><span class="o">&lt;</span>
				<span class="n">BLOCK_SIZE</span><span class="o">&gt;&lt;&lt;&lt;</span>
				<span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="n">BLOCK_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span>
				<span class="n">BLOCK_SIZE</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
				<span class="n">n</span><span class="p">,</span>
				<span class="n">thrust</span><span class="o">::</span><span class="n">raw_pointer_cast</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="n">data</span><span class="p">()),</span>
				<span class="n">thrust</span><span class="o">::</span><span class="n">raw_pointer_cast</span><span class="p">(</span><span class="n">tmp</span><span class="p">.</span><span class="n">data</span><span class="p">()));</span>
			<span class="n">sum</span> <span class="o">=</span> <span class="n">thrust</span><span class="o">::</span><span class="n">reduce</span><span class="p">(</span><span class="n">tmp</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">tmp</span><span class="p">.</span><span class="n">begin</span><span class="p">()</span> <span class="o">+</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="n">BLOCK_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCK_SIZE</span><span class="p">);</span>
		<span class="p">}</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">op</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span>
		<span class="p">{</span>
			<span class="n">warp_asum_kernel</span><span class="o">&lt;</span>
				<span class="n">WARP_SIZE</span><span class="o">&gt;&lt;&lt;&lt;</span>
				<span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="n">BLOCK_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span>
				<span class="n">BLOCK_SIZE</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
				<span class="n">n</span><span class="p">,</span>
				<span class="n">thrust</span><span class="o">::</span><span class="n">raw_pointer_cast</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="n">data</span><span class="p">()),</span>
				<span class="n">thrust</span><span class="o">::</span><span class="n">raw_pointer_cast</span><span class="p">(</span><span class="n">tmp</span><span class="p">.</span><span class="n">data</span><span class="p">()));</span>
			<span class="n">sum</span> <span class="o">=</span> <span class="n">thrust</span><span class="o">::</span><span class="n">reduce</span><span class="p">(</span><span class="n">tmp</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">tmp</span><span class="p">.</span><span class="n">begin</span><span class="p">()</span> <span class="o">+</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="n">WARP_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">WARP_SIZE</span><span class="p">);</span>
		<span class="p">}</span>
		<span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">end</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
		<span class="n">cudaEventSynchronize</span><span class="p">(</span><span class="n">beg</span><span class="p">);</span>
		<span class="n">cudaEventSynchronize</span><span class="p">(</span><span class="n">end</span><span class="p">);</span>
		<span class="kt">float</span> <span class="n">elapsed_time</span><span class="p">;</span>
		<span class="n">cudaEventElapsedTime</span><span class="p">(</span>
			<span class="o">&amp;</span><span class="n">elapsed_time</span><span class="p">,</span>
			<span class="n">beg</span><span class="p">,</span>
			<span class="n">end</span><span class="p">);</span>
		<span class="n">printf</span><span class="p">(</span><span class="s">"%u : %fms elapsed.</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">sum</span><span class="p">,</span> <span class="n">elapsed_time</span><span class="p">);</span>
	<span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<h2 id="å‚è€ƒèµ„æ–™">å‚è€ƒèµ„æ–™</h2>

<ul>
  <li><a href="https://devblogs.nvidia.com/using-cuda-warp-level-primitives/">Using CUDA Warp-Level Primitives | NVIDIA Developer Blog</a></li>
  <li><a href="https://developer.download.nvidia.cn/assets/cuda/files/reduction.pdf">Optimizing Parallel Reduction in CUDA | NVIDIA Developer Technology</a></li>
</ul>
:ET
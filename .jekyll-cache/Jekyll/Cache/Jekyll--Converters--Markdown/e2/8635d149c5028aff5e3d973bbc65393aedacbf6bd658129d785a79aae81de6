I"±¬<p>å›¾å·ç§¯ç½‘ç»œï¼ˆGraph Convolutional Network, GCNï¼‰å‘Šè¯‰æˆ‘ä»¬å°†å±€éƒ¨çš„å›¾ç»“æ„å’ŒèŠ‚ç‚¹ç‰¹å¾ç»“åˆå¯ä»¥åœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸­è·å¾—ä¸é”™çš„è¡¨ç°ï¼›å›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆGraph Attention Network, GATï¼‰åˆ™åœ¨ GCN çš„åŸºç¡€ä¹‹ä¸Šå¼•å…¥äº†æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œå…‹æœäº†å…ˆå‰å›¾å·ç§¯ç½‘ç»œçš„çŸ­æ¿ã€‚ä¸‹é¢ä¸‰ç¯‡è®ºæ–‡æœ‰é€’è¿›å…³ç³»ï¼Œè¿™ç¯‡ç¬”è®°ç€é‡æ•´ç†å…¶ä¸­ç¬¬äºŒç¯‡è®ºæ–‡ï¼š</p>

<ol>
  <li><a href="https://arxiv.org/abs/1609.02907">Semi-Supervised Classification with Graph Convolutional Networks</a>ï¼ŒICLR 2017ï¼Œå›¾å·ç§¯ç½‘ç»œ</li>
  <li><a href="https://arxiv.org/abs/1710.10903">Graph Attention Network</a>ï¼ŒICLR 2018ï¼Œå›¾æ³¨æ„åŠ›ç½‘ç»œ</li>
  <li><a href="https://arxiv.org/abs/1904.05811">Relational Graph Attention Networks</a>ï¼ŒICLR2019ï¼Œå…³è”æ€§å›¾æ³¨æ„åŠ›ç½‘ç»œï¼Œæ•´åˆäº† GCN+Attention+Relational</li>
</ol>

<h2 id="æ–¹æ³•ä»‹ç»">æ–¹æ³•ä»‹ç»</h2>

<p>é’ˆå¯¹æ¯ä¸€ä¸ªèŠ‚ç‚¹è¿ç®—ç›¸åº”çš„éšè—ä¿¡æ¯ï¼Œåœ¨è¿ç®—å…¶ç›¸é‚»èŠ‚ç‚¹çš„æ—¶å€™å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰å¦‚ä¸‹å‡ ä¸ªç‰¹æ€§ä¸ä¼˜åŠ¿ï¼š</p>

<ul>
  <li>é«˜æ•ˆï¼Œä¸ GCN çš„å¤æ‚åº¦ç›¸å½“ï¼Œå¹¶ä¸”å¯ä»¥å¹¶è¡Œè¿ç®—ã€‚</li>
  <li>é’ˆå¯¹æœ‰ä¸åŒåº¦æ•°ï¼ˆdegreeï¼‰çš„èŠ‚ç‚¹ï¼Œå¯ä»¥è¿ç”¨ä»»æ„å¤§å°çš„æƒé‡ä¸ä¹‹å¯¹åº”ã€‚</li>
  <li>å¯ç”¨äºå½’çº³å­¦ä¹ ï¼ˆinductive learningï¼‰ï¼Œæ–°æ¥èŠ‚ç‚¹çš„æ—¶å€™ï¼Œåªéœ€è¦è€ƒè™‘é‚»å±…çš„ä¿¡æ¯ï¼Œå¯¹ GAT çš„å½±å“å¹¶ä¸å¤§ï¼ˆä½¿ç”¨ GCN æ—¶æ•´ä¸ªå›¾çŸ©é˜µå°±å‘ç”Ÿäº†å˜åŒ–ï¼Œéœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ï¼‰</li>
</ul>

<p>åœ¨å››ä¸ªæ•°æ®é›†ï¼ˆCoraã€Citeseerã€Pubmedã€protein interactionï¼‰ä¸Šè¾¾åˆ° state of the art çš„å‡†ç¡®ç‡ã€‚</p>

<h3 id="å›¾æ³¨æ„åŠ›å±‚graph-attention-layer">å›¾æ³¨æ„åŠ›å±‚ï¼ˆGraph Attention Layerï¼‰</h3>

<p>è¾“å…¥ $\mathbf{h}$ ä¸º $N$ ä¸ªèŠ‚ç‚¹çš„æ¯ä¸ªèŠ‚ç‚¹çš„ $F$ ä¸ªç‰¹å¾ã€‚</p>

\[\mathbf{h}=\{\vec{h_1},\vec{h_2},\ldots,\vec{h_N}\},\vec{h_i}\in \reals^F\]

<p>è¾“å‡º $\mathbf{hâ€™}$ ä¸º $N$ ä¸ªèŠ‚ç‚¹çš„ $Fâ€™$ ä¸ªç‰¹å¾ã€‚</p>

\[\mathbf{h'}=\{\vec{h_1'},\vec{h_2'},\ldots,\vec{h_N'}\},\vec{h_i'}\in \reals^{F'}\]

<p>ä¸ºäº†å¾—åˆ°ç›¸åº”çš„è¾“å…¥ä¸è¾“å‡ºçš„è½¬æ¢ï¼Œéœ€è¦æ ¹æ®è¾“å…¥çš„ç‰¹å¾è‡³å°‘ä¸€æ¬¡çº¿æ€§å˜æ¢å¾—åˆ°è¾“å‡ºçš„ç‰¹å¾ï¼Œæ‰€ä»¥éœ€è¦å¯¹æ‰€æœ‰èŠ‚ç‚¹è®­ç»ƒä¸€ä¸ªæƒå€¼çŸ©é˜µ $\mathbf{W}\in\reals^{F\times Fâ€™}$ï¼Œè¿™ä¸ªæƒå€¼çŸ©é˜µå°±æ˜¯è¾“å…¥ä¸è¾“å‡ºçš„ $F$ ä¸ªç‰¹å¾ä¸è¾“å‡ºçš„ $Fâ€™$ ä¸ªç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚</p>

<p>ä½œè€…é’ˆå¯¹æ¯ä¸ªèŠ‚ç‚¹å®è¡Œ self-attention çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœºåˆ¶ä¸º $a:\reals^{Fâ€™}\times\reals^{Fâ€™}\to \reals$ï¼Œæ³¨æ„åŠ›äº’ç›¸å…³ç³»æ•°ï¼ˆattention coefficientsï¼‰ä¸º</p>

\[e_{ij}=a(\mathbf{W}\vec{h_i},\mathbf{W}\vec{h_j})\]

<p>è¡¨ç¤ºèŠ‚ç‚¹ $j$ å¯¹äºèŠ‚ç‚¹ $i$ çš„é‡è¦æ€§ï¼ˆä¸è€ƒè™‘å›¾ç»“æ„æ€§çš„ä¿¡æ¯ï¼‰ã€‚</p>

<p>é€šè¿‡ masked attentionï¼ˆåªè®¡ç®—èŠ‚ç‚¹ $i$ ç›¸é‚»èŠ‚ç‚¹ $j\in \mathcal{N}_i$ï¼Œå…¶ä¸­ $\mathcal{N}_i$ è¡¨ç¤º $i$ ç›¸é‚»èŠ‚ç‚¹æ„æˆçš„é›†åˆï¼‰å°†è¿™ä¸ªæ³¨æ„åŠ›æœºåˆ¶å¼•å…¥å›¾ç»“æ„ä¹‹ä¸­ã€‚ä¸ºäº†ä½¿å¾—äº’ç›¸å…³ç³»æ•°æ›´å®¹æ˜“è®¡ç®—å’Œä¾¿äºæ¯”è¾ƒï¼Œå¼•å…¥äº† $\text{softmax}$ å¯¹æ‰€æœ‰çš„ $j\in \mathcal{N}_i$ è¿›è¡Œæ­£åˆ™åŒ–ï¼š</p>

\[\alpha_{ij}=\text{softmax}_j(e_{ij})=\frac{\exp(e_{ij})}{\sum_{k\in\mathcal{N}_i}\exp(e_{ik})}\]

<p>å®éªŒä¹‹ä¸­ï¼Œæ³¨æ„åŠ›æœºåˆ¶ $a$ æ˜¯ä¸€ä¸ªå•å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œé€šè¿‡æƒå€¼å‘é‡æ¥ç¡®å®šæ¨¡å‹æƒé‡ $\vec{a}\in \reals^{2Fâ€™}$ï¼Œå¹¶ä¸”åŠ å…¥äº† $\text{LeakyRelu}$ çš„éçº¿æ€§æ¿€æ´»ï¼Œå°äºé›¶æ–œç‡ä¸º 0.2ï¼Œå¤§äº 0 æ–œç‡ä¸º 0.1ã€‚äºæ˜¯éœ€è¦æ±‚çš„æ³¨æ„åŠ›äº’ç›¸å…³ç³»æ•°çš„å®Œæ•´è¡¨è¾¾å¼å¦‚ä¸‹ï¼š</p>

\[\alpha_{ij}=\frac{
    \exp(
        \text{LeakyRelu}(\vec{a}^T[\mathbf{W}\vec{h_i}\parallel\mathbf{W}\vec{h_j}])
    )
}{
    \sum_{k\in\mathcal{N}_i}\exp(
        \text{LeakyRelu}(\vec{a}^T[\mathbf{W}\vec{h_i}\parallel\mathbf{W}\vec{h_k}])
    )
}\]

<p>å…¶ä¸­ $^T$ ä»£è¡¨è½¬ç½®ï¼Œ$\parallel$ è¡¨ç¤ºå¹¶åˆ—ï¼ˆconcatenationï¼‰è¿ç®—ã€‚</p>

<p><img src="https://cdn.jsdelivr.net/gh/dsgiitr/graph_nets/GAT/img/Attentional_Layer.jpg" alt="Attentional_Layer" /></p>

<p>å…¬å¼å«ä¹‰å°±æ˜¯æƒå€¼çŸ©é˜µä¸ $Fâ€™$ ä¸ªç‰¹å¾ç›¸ä¹˜ï¼Œç„¶åèŠ‚ç‚¹ç›¸ä¹˜åå¹¶åˆ—åœ¨ä¸€èµ·ï¼Œä¸æƒé‡ç›¸ä¹˜ï¼Œ$\text{LeakyRelu}$ æ¿€æ´»åæŒ‡æ•°æ“ä½œå¾—åˆ° $\text{softmax}$ çš„åˆ†å­ï¼ˆåˆ†æ¯åŒç†ï¼‰ã€‚</p>

<p>å€¼å¾—ä¸€æçš„æ˜¯ï¼Œè¿™ç¯‡è®ºæ–‡çš„æ—©æœŸç‰ˆæœ¬é‡Œæ²¡æœ‰ä½¿ç”¨ $\text{LeakyRelu}$ è¿›è¡Œæ¿€æ´»æ“ä½œã€‚å¯¹äºå¯¹äºæ¯æ¡è¾¹$(i,j)$ï¼Œåˆ†åˆ«å°†èŠ‚ç‚¹ $i$ å’Œ $j$ çš„ç‰¹å¾å˜æ¢ä¸º 1 ä¸ªæ ‡é‡ $f(i)$ å’Œ $f(j)$ ï¼Œç„¶åç›¸åŠ ä½œä¸ºæœªå½’ä¸€åŒ–æƒé‡ã€‚ç„¶è€Œåç»­è¿›è¡Œ $\text{softmax}$ æ—¶ï¼Œä¼ å…¥å‡½æ•°çš„å½¢å¼å¦‚ä¸‹ï¼š</p>

\[\frac{
    \exp(f(i)+f(j))
}{
    \sum_{k\in\mathcal{N}_i}\exp(f(i)+f(k))
}\]

<p>çº¦å» $\exp(f(i))$ åå¾—åˆ°</p>

\[\frac{
    \exp(f(j))
}{
    \sum_{k\in\mathcal{N}_i}\exp(f(k))
}\]

<p>è¿™è¡¨æ˜é‚»èŠ‚ç‚¹çš„é‡è¦æ€§ä¸å½“å‰èŠ‚ç‚¹æ— å…³ï¼Œåªä¸é‚»èŠ‚ç‚¹è‡ªèº«ç‰¹å¾æœ‰å…³ã€‚è¿™æ ·å¿½è§†äº†èŠ‚ç‚¹æœ¬èº«ï¼Œå°±å®¹æ˜“å¯¼è‡´çŸ©é˜µåœ¨è¡Œä¸Šç›¸ä¼¼ç”šè‡³ç›¸ç­‰ï¼Œæ˜¾ç„¶æ˜¯æœ‰é—®é¢˜çš„ã€‚å› æ­¤ï¼Œæˆ‘æ¨æµ‹æ–°ç‰ˆè®ºæ–‡é‡ŒåŠ äº†æ¿€æ´»å‡½æ•°å¯ç”¨äºç ´åä¸Šé¢çš„å…¬å¼ï¼Œä½¿å¾— $i$ ä¸èƒ½çº¦å»ã€‚</p>

<p>é€šè¿‡ä¸Šé¢ï¼Œè¿ç®—å¾—åˆ°äº†æ­£åˆ™åŒ–åçš„ä¸åŒèŠ‚ç‚¹ä¹‹é—´çš„æ³¨æ„åŠ›äº’ç›¸å…³ç³»æ•°ï¼ˆnormalized attention coefficientsï¼‰ï¼Œå¯ä»¥ç”¨æ¥é¢„æµ‹æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºç‰¹å¾ï¼š</p>

\[\vec{h_i'}=\sigma(\sum_{j\in\mathcal{N}_i}\alpha_{ij}\mathbf{W}\vec{h_j})\]

<p>è¿™ä¸ªå…¬å¼çš„å«ä¹‰å°±æ˜¯èŠ‚ç‚¹çš„è¾“å‡ºç‰¹å¾æ˜¯ç›¸é‚»çš„æ‰€æœ‰èŠ‚ç‚¹çš„çº¿æ€§å’Œçš„éçº¿æ€§æ¿€æ´»ï¼Œçº¿æ€§å’Œçš„ç³»æ•°æ˜¯å‰é¢æ±‚å¾—çš„æ³¨æ„åŠ›äº’ç›¸å…³ç³»æ•°ã€‚</p>

<p>åœ¨ä¸Šé¢çš„è¾“å‡ºç‰¹å¾åŠ å…¥è®¡ç®— multi-head çš„è¿ç®—å…¬å¼:</p>

\[\vec{h_i'}=\parallel_{k=1}^K\sigma(\sum_{j\in \mathcal{N}_i}\alpha_{ij}^k\mathbf{W}^k\vec{h_j})\]

<p>å…¶ä¸­ concate æ“ä½œä¸º $\parallel$ ï¼Œ$a_{ij}^k$ æ˜¯ç¬¬ $k$ ä¸ªæ³¨æ„åŠ›æœºåˆ¶ $(a_k)$ ç®—å‡ºçš„æ­£åˆ™åŒ–æ³¨æ„åŠ›äº’ç›¸å…³ç³»æ•°ï¼Œ$\mathbf{W}^k$ æ˜¯å¯¹åº”çš„æƒé‡çŸ©é˜µï¼Œå…± $K$ ä¸ªæ³¨æ„åŠ›æœºåˆ¶éœ€è¦è€ƒè™‘ï¼Œæ¯ä¸ªèŠ‚ç‚¹è¾“å‡º $KFâ€™$ ä¸ªç‰¹å¾ã€‚ä¾‹å¦‚ï¼Œ$K=3$ æ—¶ç»“æ„å¦‚ä¸‹ï¼ŒèŠ‚ç‚¹ 1 åœ¨é‚»åŸŸä¸­å…·æœ‰å¤šç«¯æ³¨æ„æœºåˆ¶ï¼Œä¸åŒçš„ç®­å¤´æ ·å¼è¡¨ç¤ºç‹¬ç«‹çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œé€šè¿‡è¿æ¥æˆ–å¹³å‡æ¯ä¸ª head è·å– $\vec{h_1â€™}$ï¼š</p>

<p><img src="https://cdn.jsdelivr.net/gh/dsgiitr/graph_nets/GAT/img/MultiHead_Attention.jpeg" alt="MultiHead_Attention" /></p>

<p>å¯¹äºæœ€ç»ˆçš„è¾“å‡ºï¼Œconcate æ“ä½œå¯èƒ½ä¸é‚£ä¹ˆæ•æ„Ÿäº†ï¼Œæ‰€ä»¥ä½œè€…ç›´æ¥ç”¨ K å¹³å‡æ¥å–ä»£ concate æ“ä½œï¼Œå¾—åˆ°æœ€ç»ˆçš„å…¬å¼ï¼š</p>

\[\vec{h_i'}=\sigma(\frac{1}{K}\sum_{k=1}^K\sum_{j\in\mathcal{N}_i}\alpha_{ij}^k\mathbf{W}^k\vec{h_j})\]

<h2 id="å®éªŒè¿‡ç¨‹">å®éªŒè¿‡ç¨‹</h2>

<p>åŸè®ºæ–‡ä¸­çš„å®éªŒåˆ†æˆä¸¤éƒ¨åˆ†ï¼ŒåŠç›‘ç£å­¦ä¹ ï¼ˆtransductive learningï¼‰å’Œå½’çº³å­¦ä¹ ï¼ˆinductive learningï¼‰ã€‚é™äºæ–‡ç« ç¯‡å¹…ï¼Œè¿™é‡Œæˆ‘åªæ”¾å‡ºåŠç›‘ç£å­¦ä¹ çš„éƒ¨åˆ†ï¼Œå½’çº³å­¦ä¹ ä¸­å¯¹äºè®ºæ–‡ä¸­æ ¸å¿ƒçš„å›¾æ³¨æ„åŠ›å±‚çš„åŸç†æ˜¯ç›¸åŒçš„ï¼Œä¹Ÿå¯ä»¥æŸ¥çœ‹æ–‡æœ«å‚è€ƒé“¾æ¥éƒ¨åˆ†åŸä½œè€…çš„ä»£ç ã€‚</p>

<h3 id="å®éªŒç¯å¢ƒ">å®éªŒç¯å¢ƒ</h3>

<p>ä½¿ç”¨çš„æœºå™¨æ²¡æœ‰ cuda ç¯å¢ƒï¼Œå¥½åœ¨å³ä½¿æ˜¯åœ¨ cpu ä¸Šä¹Ÿèƒ½å¾ˆå¿«ï¼ˆ1 åˆ†é’Ÿå·¦å³ï¼‰è®­ç»ƒå®Œè¿™ä¸ªæ¨¡å‹ã€‚</p>

<ul>
  <li>Intel(R) Core(TM) i7-6567U CPU @3.30GHZ 3.31GHz</li>
  <li>8.00GB RAM</li>
  <li>Python 3.8.2 64-bit
    <ul>
      <li>jupyter==1.0.0</li>
      <li>numpy==1.18.4</li>
      <li>torch==1.5.0+cpu</li>
      <li>torch-scatter==2.0.4</li>
      <li>torch-sparse==0.6.4</li>
      <li>torch-cluster==1.5.4</li>
      <li>torch-geometric==1.5.0</li>
    </ul>
  </li>
</ul>

<h3 id="å¯¼å…¥ç›¸å…³åŒ…å’Œæ•°æ®é›†">å¯¼å…¥ç›¸å…³åŒ…å’Œæ•°æ®é›†</h3>

<p>è¿™é‡Œæˆ‘æ²¡æœ‰é€‰ç”¨åŸä½œè€…çš„ä»£ç ï¼ˆè§å‚è€ƒéƒ¨åˆ†ï¼‰ï¼Œè€Œæ˜¯ä½¿ç”¨äº† PyTorch Geometric åº“ï¼Œå…¶ä¼˜ç‚¹æ˜¯è®¾è®¡äº†ä¸€ç§æ–°çš„è¡¨ç¤ºå›¾æ•°æ®çš„å­˜å‚¨ç»“æ„ï¼Œåªä¼šä¸ºå­˜åœ¨çš„è¾¹è¿›è¡Œè®¡ç®—ï¼Œè€Œå®Œå…¨ä¸éœ€è¦å°†è®¡ç®—æµªè´¹åœ¨ä¸å­˜åœ¨çš„è¾¹ä¸Šã€‚ç›¸å¯¹äºåŸä½œè€…ç›´æ¥ä½¿ç”¨çš„é‚»æ¥çŸ©é˜µï¼ˆæµªè´¹å¤§é‡çš„æ—¶é—´ä¸ºæ²¡æœ‰è¾¹çš„èŠ‚ç‚¹å¯¹è®¡ç®—æƒé‡ï¼Œç„¶è€Œåœ¨ç¬¬äºŒæ­¥ä¸­è¢« Mask æ¶ˆå»ï¼‰ï¼Œåœ¨æˆ‘çš„æœºå™¨ä¸Šå¯ä»¥æŠŠè®­ç»ƒæ¨¡å‹çš„æ—¶é—´ç¼©çŸ­åˆ°ä¸€åˆ†é’Ÿä»¥å†…ï¼ˆCora æ•°æ®é›†ï¼‰ã€‚</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch_geometric.data</span> <span class="kn">import</span> <span class="n">Data</span>
<span class="kn">from</span> <span class="nn">torch_geometric.datasets</span> <span class="kn">import</span> <span class="n">Planetoid</span>
<span class="kn">from</span> <span class="nn">torch_geometric</span> <span class="kn">import</span> <span class="n">transforms</span>

<span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">2020</span><span class="p">)</span> <span class="c1"># seed for reproducible numbers
</span><span class="n">name_data</span> <span class="o">=</span> <span class="s">'Cora'</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">Planetoid</span><span class="p">(</span><span class="n">root</span><span class="o">=</span> <span class="s">'./dataset/'</span> <span class="o">+</span> <span class="n">name_data</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="n">name_data</span><span class="p">)</span>
<span class="n">dataset</span><span class="p">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">NormalizeFeatures</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">f"Number of Classes in </span><span class="si">{</span><span class="n">name_data</span><span class="si">}</span><span class="s">:"</span><span class="p">,</span> <span class="n">dataset</span><span class="p">.</span><span class="n">num_classes</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">f"Number of Node Features in </span><span class="si">{</span><span class="n">name_data</span><span class="si">}</span><span class="s">:"</span><span class="p">,</span> <span class="n">dataset</span><span class="p">.</span><span class="n">num_node_features</span><span class="p">)</span>
</code></pre></div></div>

<p>æ³¨æ„åˆ°ä¸‹è¿°ä»£ç ä¼šç›´æ¥è®¿é—® <a href="https://github.com/kimiyoung/planetoid">kimiyoung/planetoid</a> ä¸‹è½½æ‰€éœ€è¦çš„æ•°æ®é›†åˆ°å½“å‰ç›®å½•ä¸‹ï¼Œå›½å†…ç½‘ç»œç¯å¢ƒä¸‹æœ‰æ—¶éœ€è¦æŒ‚ä¸Šä»£ç†ï¼Œå¦åˆ™ä¼šåœ¨è¿è¡Œæ—¶æŠ¥â€œè¿æ¥æ–­å¼€â€çš„é”™è¯¯ã€‚</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Number of Classes <span class="k">in </span>Cora: 7
Number of Node Features <span class="k">in </span>Cora: 1433
</code></pre></div></div>

<h3 id="å›¾æ³¨æ„åŠ›å±‚">å›¾æ³¨æ„åŠ›å±‚</h3>

<p><code class="language-plaintext highlighter-rouge">torch_geometric</code>åŒ…å·²ç»å®ç°äº†æ‰€éœ€çš„å›¾æ³¨æ„åŠ›å›¾å±‚ï¼Œä»¥ä¸‹å†…å®¹å¯ä»¥ç”±ä¸€å¥ <code class="language-plaintext highlighter-rouge">from torch_geometric.nn import GATConv</code> ç®€å•ä»£æ›¿ã€‚ä½†æ˜¯ä½œä¸ºå¯¹è¿™ç¯‡è®ºæ–‡æœ¬èº«çš„å­¦ä¹ ï¼Œè¿˜æ˜¯æœ‰å¿…è¦è´´å‡ºå¯¹åº”çš„<a href="https://github.com/rusty1s/pytorch_geometric/blob/master/torch_geometric/nn/conv/gat_conv.py">æºç </a>ã€‚</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># https://github.com/rusty1s/pytorch_geometric/blob/master/torch_geometric/nn/conv/gat_conv.py
</span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">Linear</span>
<span class="kn">from</span> <span class="nn">torch_geometric.nn.conv</span> <span class="kn">import</span> <span class="n">MessagePassing</span>
<span class="kn">from</span> <span class="nn">torch_geometric.utils</span> <span class="kn">import</span> <span class="n">remove_self_loops</span><span class="p">,</span> <span class="n">add_self_loops</span><span class="p">,</span> <span class="n">softmax</span>

<span class="kn">from</span> <span class="nn">torch_geometric.nn.inits</span> <span class="kn">import</span> <span class="n">glorot</span><span class="p">,</span> <span class="n">zeros</span>

<span class="k">class</span> <span class="nc">GATConv</span><span class="p">(</span><span class="n">MessagePassing</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">concat</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                 <span class="n">negative_slope</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GATConv</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">aggr</span><span class="o">=</span><span class="s">'add'</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">concat</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">negative_slope</span> <span class="o">=</span> <span class="n">negative_slope</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">__alpha__</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">lin</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">heads</span> <span class="o">*</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">att_i</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">heads</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">att_j</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">heads</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">bias</span> <span class="ow">and</span> <span class="n">concat</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">heads</span> <span class="o">*</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">bias</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">concat</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">out_channels</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s">'bias'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">glorot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lin</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">glorot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">att_i</span><span class="p">)</span>
        <span class="n">glorot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">att_j</span><span class="p">)</span>
        <span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">return_attention_weights</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="s">""""""</span>

        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lin</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="bp">self</span><span class="p">.</span><span class="n">lin</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

        <span class="n">edge_index</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">remove_self_loops</span><span class="p">(</span><span class="n">edge_index</span><span class="p">)</span>
        <span class="n">edge_index</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">add_self_loops</span><span class="p">(</span><span class="n">edge_index</span><span class="p">,</span>
                                       <span class="n">num_nodes</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">size</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">node_dim</span><span class="p">))</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">propagate</span><span class="p">(</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
                             <span class="n">return_attention_weights</span><span class="o">=</span><span class="n">return_attention_weights</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">concat</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">heads</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span>

        <span class="k">if</span> <span class="n">return_attention_weights</span><span class="p">:</span>
            <span class="n">alpha</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">__alpha__</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">__alpha__</span><span class="p">,</span> <span class="bp">None</span>
            <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="p">(</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">message</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_i</span><span class="p">,</span> <span class="n">x_j</span><span class="p">,</span> <span class="n">edge_index_i</span><span class="p">,</span> <span class="n">size_i</span><span class="p">,</span>
                <span class="n">return_attention_weights</span><span class="p">):</span>
        <span class="c1"># Compute attention coefficients.
</span>        <span class="n">x_i</span> <span class="o">=</span> <span class="n">x_i</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="n">x_j</span> <span class="o">=</span> <span class="n">x_j</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">out_channels</span><span class="p">)</span>

        <span class="n">alpha</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_i</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">att_i</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_j</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">att_j</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">negative_slope</span><span class="p">)</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">edge_index_i</span><span class="p">,</span> <span class="n">size_i</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_attention_weights</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">__alpha__</span> <span class="o">=</span> <span class="n">alpha</span>

        <span class="c1"># Sample attention coefficients stochastically.
</span>        <span class="n">alpha</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x_j</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>æˆ‘ä»¬åªéœ€è¦å…³æ³¨å…¶ <code class="language-plaintext highlighter-rouge">forward</code> è¿‡ç¨‹ä¸­ä¼šè°ƒç”¨çš„ <code class="language-plaintext highlighter-rouge">message</code> æ–¹æ³•ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">alpha</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_i</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">att_i</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_j</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">att_j</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">negative_slope</span><span class="p">)</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">edge_index_i</span><span class="p">,</span> <span class="n">size_i</span><span class="p">)</span>

<span class="c1"># Sample attention coefficients stochastically.
</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>

<span class="k">return</span> <span class="n">x_j</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>è¿™é‡Œç®—å‡ºäº†å‰é¢è®ºæ–‡ä¸­æåˆ°çš„ $\alpha_{ij}$ï¼Œç„¶åè¿›è¡Œ dropout æ“ä½œï¼Œç›¸å½“äºè®¡ç®—æ¯ä¸ª node ä½ç½®çš„å·ç§¯æ—¶éƒ½æ˜¯éšæœºçš„é€‰å–äº†ä¸€éƒ¨åˆ†è¿‘é‚»èŠ‚ç‚¹å‚ä¸å·ç§¯ï¼Œå¯ä»¥æ¯”è¾ƒæœ‰æ•ˆçš„ç¼“è§£è¿‡æ‹Ÿåˆçš„å‘ç”Ÿï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šè¾¾åˆ°æ­£åˆ™åŒ–çš„æ•ˆæœã€‚æœ€åè¿”å› dropout åå·ç§¯çš„ç»“æœã€‚</p>

<h3 id="æ¨¡å‹">æ¨¡å‹</h3>

<ul>
  <li>ä¸¤å±‚ GATLayer</li>
  <li>ç¬¬ä¸€å±‚ 8 headï¼Œ$Fâ€™=8$ï¼Œä½¿ç”¨ eluï¼ˆexponential linear unitï¼‰ä½œä¸ºéçº¿æ€§å‡½æ•°</li>
  <li>ç¬¬äºŒå±‚ä¸ºåˆ†ç±»å±‚ï¼Œä¸€ä¸ª attention headï¼Œç‰¹å¾æ•°å°±æ˜¯ç±»åˆ«æ•°ï¼Œåé¢è·Ÿç€ $\text{softmax}$ å‡½æ•°</li>
  <li>ä¸¤å±‚éƒ½é‡‡ç”¨ 0.6 çš„ dropout</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GAT</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GAT</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hid</span> <span class="o">=</span> <span class="mi">8</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">in_head</span> <span class="o">=</span> <span class="mi">8</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_head</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">GATConv</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">num_features</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">hid</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">in_head</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">GATConv</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hid</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">in_head</span><span class="p">,</span> <span class="n">dataset</span><span class="p">.</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">concat</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                             <span class="n">heads</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">out_head</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">edge_index</span>

        <span class="c1"># Dropout before the GAT layer is used to avoid overfitting in small datasets like Cora.
</span>        <span class="c1"># One can skip them if the dataset is sufficiently large.
</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">elu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="è®­ç»ƒ">è®­ç»ƒ</h3>

<p>ä¸ºäº†åº”å¯¹æ•°æ®é›†å°çš„é—®é¢˜ï¼Œè®­ç»ƒæ—¶ä½¿ç”¨äº† <code class="language-plaintext highlighter-rouge">weight_decay=5e-4</code> çš„ $L_2$ æ­£åˆ™åŒ–ã€‚</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GAT</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="n">data</span><span class="p">.</span><span class="n">train_mask</span><span class="p">],</span> <span class="n">data</span><span class="p">.</span><span class="n">y</span><span class="p">[</span><span class="n">data</span><span class="p">.</span><span class="n">train_mask</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">epoch</span><span class="o">%</span><span class="mi">200</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor<span class="o">(</span>1.9447, <span class="nv">grad_fn</span><span class="o">=</span>&lt;NllLossBackward&gt;<span class="o">)</span>
tensor<span class="o">(</span>0.6848, <span class="nv">grad_fn</span><span class="o">=</span>&lt;NllLossBackward&gt;<span class="o">)</span>
tensor<span class="o">(</span>0.6022, <span class="nv">grad_fn</span><span class="o">=</span>&lt;NllLossBackward&gt;<span class="o">)</span>
tensor<span class="o">(</span>0.5589, <span class="nv">grad_fn</span><span class="o">=</span>&lt;NllLossBackward&gt;<span class="o">)</span>
tensor<span class="o">(</span>0.4930, <span class="nv">grad_fn</span><span class="o">=</span>&lt;NllLossBackward&gt;<span class="o">)</span>
</code></pre></div></div>

<h3 id="è¯„ä¼°æ•ˆæœ">è¯„ä¼°æ•ˆæœ</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
<span class="n">_</span><span class="p">,</span> <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="nb">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">correct</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">pred</span><span class="p">[</span><span class="n">data</span><span class="p">.</span><span class="n">test_mask</span><span class="p">].</span><span class="n">eq</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">y</span><span class="p">[</span><span class="n">data</span><span class="p">.</span><span class="n">test_mask</span><span class="p">]).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">())</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">data</span><span class="p">.</span><span class="n">test_mask</span><span class="p">.</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuracy: {:.4f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">acc</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy: 0.8210
</code></pre></div></div>

<h2 id="æ€»ç»“">æ€»ç»“</h2>

<p>GAT å…¶å®å°±æ˜¯ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥è®¡ç®—èšåˆå‘¨è¾¹èŠ‚ç‚¹æ—¶çš„æƒé‡ã€‚å—ç›Šäºæ³¨æ„åŠ›æœºåˆ¶ï¼ŒGAT èƒ½å¤Ÿè¿‡æ»¤å™ªéŸ³é‚»å±…ï¼Œæ”¹è¿›å›¾å·ç§¯çš„ç¼ºç‚¹æå‡æ¨¡å‹è¡¨ç°å¹¶å¯ä»¥å¯¹ç»“æœå®ç°ä¸€å®šçš„è§£é‡Šã€‚æ­¤å¤–ï¼ŒGAT ä¸éœ€è¦äº‹å…ˆçŸ¥é“å›¾ç»“æ„ï¼Œä¹Ÿå¯ä»¥åº”ç”¨åœ¨åŠ¨æ€å›¾ä¸Šï¼Œæ— ç–‘å¤§å¤§æ‰©å±•äº† GCN çš„é€‚ç”¨é¢ã€‚</p>

<h2 id="ä¸»è¦å‚è€ƒæ–‡çŒ®">ä¸»è¦å‚è€ƒæ–‡çŒ®</h2>

<ul>
  <li><a href="https://arxiv.org/abs/1710.10903">Graph Attention Network</a></li>
  <li><a href="https://github.com/Diego999/pyGAT">Diego999/pyGAT</a></li>
  <li><a href="https://github.com/dsgiitr/graph_nets/blob/master/GAT/GAT.md">Understanding Graph Attention Networks (GAT)</a></li>
  <li><a href="https://zhuanlan.zhihu.com/p/132497231">æ·±å…¥ç†è§£å›¾æ³¨æ„åŠ›æœºåˆ¶</a></li>
  <li><a href="https://blog.csdn.net/weixin_36474809/article/details/89401552">å›¾æ³¨æ„åŠ›ç½‘ç»œ(GAT) ICLR2018, Graph Attention Networkè®ºæ–‡è¯¦è§£</a></li>
</ul>
:ET